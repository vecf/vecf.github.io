<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>Autonomous Vehicles - Francois van Eeden</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
	<noscript>
		<link rel="stylesheet" href="assets/css/noscript.css" />
	</noscript>
</head>

<body class="is-preload">
	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Header -->
		<header id="header">
			<div class="inner">

				<!-- Logo -->
				<a href="index.html" class="logo">
					<span class="symbol"><img src="images/logo.svg" alt="" /></span><span class="title">Autonomous Vehicles</span>
				</a>

				<!-- Nav -->
				<nav>
					<ul style="float:right;">
						<li><a href="media/resume.pdf">Francois van Eeden</a></li>
						<li><a href="https://github.com/vecf" class="icon brands style2 fa-github"><span
									class="label">GitHub</span></a></li>
						<li><a href="https://www.linkedin.com/in/francois-van-eeden-449a1395/"
								class="icon brands style2 fa-linkedin"><span class="label">LinkedIn</span></a></li>
						<li><a href="#menu">Menu</a></li>
					</ul>
				</nav>

			</div>
		</header>

		<!-- Menu -->
		<nav id="menu">
			<h2>Navigation</h2>
			<ul>
				<li><a href="index.html">Home</a></li>
				<li><a href="pebbles.html">Multi-agent Coordination</a></li>
				<li><a href="avproject.html">Autonomous Vehicles</a></li>
				<li><a href="learning.html">Study Projects</a></li>
			</ul>
		</nav>

		<!-- Main -->
		<div id="main">
			<div class="inner">

				<body> <h1 id="introduction">Introduction</h1>
				
				<p>The goal of this project was to determine to what extent yard
				tractors (or ‘port haulers’ colloquially) can be converted into
				autonomous vehicles in-house. These vehicles haul container carrying
				trailers from quayside to storage yard in the ports. The company was
				already developing a non-autonomous port hauler, but the team
				anticipated that autonomy would eventually become an entry to market
				requirement, so I decided to initiate this project. By developing an
				autonomy kit, retrofitting would also be made possible on older
				vehicles.</p>
				
				<h1 id="software-development-approach">Software Development
				Approach</h1>
				
				<p>Early on a decision had to be made whether to develop from scratch or
				leverage open source. I was on the project full-time, while another 
				engineer had half of his time dedicated to this project and others
				assisted on an ad-hoc basis. Given the small team, limited resources and
				the availability of the open source <a href="https://ros.org/">Robot
				Operating System (ROS) framework</a>, I decided to pursue the open
				source route.  This decision yielded rapid progress, greater number of
				features and higher quality software.  Exposure to an established
				codebase and community yielded secondary benefits such as adoption of
				best practices, broader applicability, and better code
				maintainability.</p>
				
				<h1 id="hardware-architecture-and-approach">Hardware Architecture and
				Approach</h1>
				
				<p>As technical lead of the project, I chose to demonstrate the
				feasibility of in-house development using a scale prototype. To keep
				costs low and development velocity high, standard off-the-shelf
				components such as remote controlled (RC) vehicles and hobby-grade
				microcontrollers were used for prototyping. The first image below shows a first
				iteration of the prototype vehicle.  Thanks to hardware modularity, this
				prototype was later easily refactored to mimic the real
				vehicle more accurately as shown in the following images (see <a
				href="https://mobile.twitter.com/te_transnet/status/1333689542969331714">
				here</a> for comparison). </p>
				
				<div class="row">
				
					<div class="col-6 col-12-medium">
						<figure>
							<img class="image fit" src="media/avproject/av_proto1.jpg">
							<figcaption>First Prototype Iteration</figcaption>
						</figure>
					</div>
				
					<div class="col-6 col-12-medium">
						<figure>
							<img class="image fit" src="media/avproject/av_proto3.jpg">
							<figcaption>Refactored Hardware</figcaption>
						</figure>
					</div>
				
				</div>

				<div class="row">

					<div class="col-6 col-12-medium">
						<figure>
							<img class="image fit" src="media/avproject/av_proto2.jpg">
							<figcaption>Second Prototype High Angle View</figcaption>
						</figure>
					</div>
				
					<div class="col-6 col-12-medium">
						<figure>
							<img class="image fit" src="media/avproject/av_proto4.jpg">
							<figcaption>Second Prototype Low Angle View</figcaption>
						</figure>
					</div>

				</div>
				
				<h1 id="simulation">Simulation</h1>
				
				<p>One strong advantage of <code>ROS</code> is its integration with <a
				href="https://gazebosim.org/"><code>gazebo</code></a> for simulation.
				By the time that the hardware was ready, most of the mapping, planning
				and control code had already been set up and tested in simulation.
				<code>Gazebo</code> simulated the vehicle odometry, laser scanner, <a
				href="https://en.wikipedia.org/wiki/Inertial_measurement_unit">IMU</a>
				and camera that were on the vehicle; in some cases, long before the
				physical hardware was available. Creating the simulations required
				developing <code>.urdf</code> files describing the robot hardware. I
				parameterized this as much as possible using <a
				href="http://wiki.ros.org/xacro"><code>.xacro</code></a> in
				anticipation for future refactoring. Hardware and software components
				were configured in the simulation phase using <a
				href="http://wiki.ros.org/roslaunch"><code>roslaunch</code></a> and
				remained handy during transfer to the physical robot. A great deal of the
				project revolved around parametrising and setting up these files, I
				refer to this as "configuration" throughout.</p>
				
				<h1 id="integrations">Integrations</h1>
				
				<p>The development process included design, fabrication and software
				development of both standard and bespoke components. Many aspects were
				successfully configured in simulation and later transferred to real
				hardware, allowing unanticipated feature additions without affecting
				quality or development velocity.  </p>
				
				<h3 id="sensors-and-sensor-fusion">Sensors and Sensor fusion</h3>
				
				<p>Since the robot had multiple data streams that were useful for
				navigation, a high quality position estimate could be obtained by fusing
				these streams using the <a
				href="http://docs.ros.org/noetic/api/robot_localization/html/index.html"><code>robot_localization</code></a>
				package, which implements a <a
				href="https://en.wikipedia.org/wiki/Kalman_filter">Kalman filter</a>.</p>
				
				<h3 id="simultaneous-localization-and-mapping-slam">Simultaneous
				Localization and Mapping (SLAM)</h3>
				
				<p>The <code>ROS</code> navigation stack uses <a
				href="https://openslam-org.github.io/">OpenSlam</a> as its default
				implementation. The real robot's main controller could not handle online
				mapping and thanks to <code>ROS</code>'s modular publish/subscribe model, it was
				simple to move the mapping node to the server side.  </p>
				
				<h3 id="path-planning">Path planning</h3>
				
				<p>The navigation stack provides a default global planner, which was
				good enough for our needs. This planner doesn’t take into account the
				vehicle kinematics and is performed against a static costmap. The
				default local planner, however, was replaced with the Timed Elastic Band
				local planner (<a
				href="http://wiki.ros.org/teb_local_planner"><code>teb_local_planner</code></a>),
				which takes the kinematics of our <a
				href="https://en.wikipedia.org/wiki/Ackermann_steering_geometry">Ackermann
				steering</a> vehicle into account.  </p>
				
				<p>Shown below is a simulation of a navigation mission for the robot.
				The target pose is shown by the purple arrow at the start of the video.
				The global planner’s path is shown in green and the local planner’s path
				in red. The output of a simulated camera as well as the simulator’s GUI
				with a bird’s eye view are shown in separate windows. There are also two
				plots showing the angular and linear velocity resulting from the
				navigation commands generated by the navigation stack. The pink/blue
				shaded area around the vehicle is the costmap taken into account by the
				local planner which is capable of handling dynamic obstacles.</p>
				
				<div class="col-12"> 
					<figure>
						<video class="video single centered" alt="Navigation Simulation Demo" controls>
							<source src="media/avproject/navSim.mp4" type="video/mp4">
						</video>
						<figcaption>Simulated Navigation Mission</figcaption>
					</figure>
				</div>
				
				<h3 id="control">Control</h3>
				
				<p>The main controller runs <code>linux</code> to support
				<code>ROS</code> and was not very powerful. This made it inadequate for
				the real-time nature of controlling motor speed. Instead, a dedicated
				low-level controller was used which ran a <a href="https://en.wikipedia.org/wiki/PID_controller">PID</a> loop and handled encoder
				pulses for speed feedback. On the main controller, the bespoke
				interfaces for <a
				href="http://wiki.ros.org/ros_control"><code>ros_control</code></a> were
				developed to pass speed and steering angle data back for integration
				into pose estimates. Getting odometry data from the standard RC motor
				encoder required splicing its signal cable and adding interrupt code to
				the low-level controller. For steering angle feedback, I soldered extra
				wires onto the servo motor control pot to read its values.  </p>
				
				<p>The main controller sent control commands to the low-level controller
				using <a
				href="http://wiki.ros.org/rosserial"><code>rosserial</code></a>. I could
				not get the <code>C++</code> <code>rosserial_server</code> to work and
				found the codebase stale at the time. Instead, we used the
				<code>rosserial_python</code> interface, which worked well from the
				get-go. Unfortunately, troubleshooting revealed that <code>teb_local_planner</code> combined with online localization and the
				high overhead of the <code>rosserial_python</code> package was too much for the embedded <code>ROS</code> controller. </p>			
				
				<h2 id="visualization-and-remote-control">Visualization and remote
				control</h2>
				
				<p>Another convenient <code>ROS</code> feature is the availability of
				various input device capabilities. A joystick, keyboards and software
				input devices were configured and used. <a
				href="http://wiki.ros.org/rviz"><code>Rviz</code></a> is yet another
				great advantage of the framework, allowing simultaneous visualization
				and interaction with real or simulated robots. The simulation
				configuration files of these tools required minimal modifications for
				use with the physical vehicle and makes the development process very
				tactile and immersive. The video below shows a mapping session using a
				joystick as input device from a remote PC. The sensor data streams are
				converted to a map of the environment computed and displayed in
				<code>rviz</code> on the remote machine using data streams relayed by the
				embedded controller. The odometry calculations are also visualized as a
				red path trailing the vehicle.</p>
				
				<div class="col-12"> 
					<figure>
						<video class="video single centered" alt="Navigation Simulation Demo" controls>
							<source src="media/avproject/mapping.mp4" type="video/mp4">
						</video>
						<figcaption>Joystick Controlled Mapping Session</figcaption>
					</figure>
				</div>

				<h1 id="conclusion">Conclusion</h1>
				
				<p>Unfortunately, the project budget was deferred before completion due
				to company-wide difficulties resulting in cost cutting and I left the
				company soon after. The project revealed that internal development would
				be possible given dedicated resources. This is largely made possible due
				to open source and the permissively licensed <code>ROS</code>. This was
				my first robot and I am proud of how far we had come and the state that
				I left the project in.</p>
				
				</body>

				</html>
			</div>
		</div>

		<!-- Footer -->
		<footer id="footer">
			<div class="inner">
				<ul class="copyright">
					<li>&copy; CF van Eeden. All rights reserved</li>
					<li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
				</ul>
			</div>
		</footer>

	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>